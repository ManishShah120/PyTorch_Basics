{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Insurance cost prediction using linear regression\n",
    "\n",
    "In this assignment we're going to use information like a person's **age**, **sex**, **BMI**, **no. of children** and **smoking habit** to **_predict the price of yearly medical bills_**. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from: https://www.kaggle.com/mirichoi0218/insurance\n",
    "\n",
    "\n",
    "We will create a model with the following steps:\n",
    "1. Download and explore the dataset\n",
    "2. Prepare the dataset for training\n",
    "3. Create a linear regression model\n",
    "4. Train the model to fit the data\n",
    "5. Make predictions using the trained model\n",
    "\n",
    "\n",
    "This assignment builds upon the concepts from the first 2 lectures. It will help to review these Jupyter notebooks:\n",
    "- PyTorch basics: https://jovian.ml/aakashns/01-pytorch-basics\n",
    "- Linear Regression: https://jovian.ml/aakashns/02-linear-regression\n",
    "- Logistic Regression: https://jovian.ml/aakashns/03-logistic-regression\n",
    "- Linear regression (minimal): https://jovian.ml/aakashns/housing-linear-minimal\n",
    "- Logistic regression (minimal): https://jovian.ml/aakashns/mnist-logistic-minimal\n",
    "\n",
    "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end . In some cases, you'll be required to choose some hyperparameters (learning rate, batch size etc.). Try to experiment with the hypeparameters to get the lowest loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the commands below if imports fail\n",
    "# !conda install numpy pytorch torchvision cpuonly -c pytorch -y\n",
    "# !pip install matplotlib --upgrade --quiet\n",
    "#!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3.7 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jovian\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='02-insurance-linear-regression' # will be used by jovian.commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and explore the data\n",
    "\n",
    "Let us begin by downloading the data. We'll use the `download_url` function from PyTorch to get the data as a CSV (comma-separated values) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./insurance.csv\n"
     ]
    }
   ],
   "source": [
    "DATASET_URL = \"https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv\"\n",
    "DATA_FILENAME = \"insurance.csv\"\n",
    "download_url(DATASET_URL, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataset into memory, we'll use the `read_csv` function from the `pandas` library. The data will be loaded as a Pandas dataframe. See this short tutorial to learn more: https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw = pd.read_csv(DATA_FILENAME)\n",
    "dataframe_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do a slight customization of the data, so that you every participant receives a slightly different version of the dataset. Fill in your name below as a string (enter at least 5 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'manishshah' # at least 5 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `customize_dataset` function will customize the dataset slightly using your name as a source of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_dataset(dataframe_raw, rand_str):\n",
    "    dataframe = dataframe_raw.copy(deep=True)\n",
    "    # drop some rows\n",
    "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
    "    # scale input\n",
    "    dataframe.bmi = dataframe.bmi * ord(rand_str[1])/100.\n",
    "    # scale target\n",
    "    dataframe.charges = dataframe.charges * ord(rand_str[2])/100.\n",
    "    # drop column\n",
    "    if ord(rand_str[3]) % 2 == 1:\n",
    "        dataframe = dataframe.drop(['region'], axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>36</td>\n",
       "      <td>male</td>\n",
       "      <td>27.73715</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>7203.014555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>51.53610</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>1279.808970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>34</td>\n",
       "      <td>male</td>\n",
       "      <td>21.74740</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>30113.495258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.54800</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>2564.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>51</td>\n",
       "      <td>male</td>\n",
       "      <td>38.50900</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>10330.480600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex       bmi  children smoker       charges\n",
       "1063   36    male  27.73715         3     no   7203.014555\n",
       "1317   18    male  51.53610         0     no   1279.808970\n",
       "140    34    male  21.74740         2     no  30113.495258\n",
       "106    19  female  27.54800         1     no   2564.670900\n",
       "634    51    male  38.50900         1     no  10330.480600"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = customize_dataset(dataframe_raw, your_name)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us answer some basic questions about the dataset. \n",
    "\n",
    "\n",
    "**Q: How many rows does the dataset have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1271 entries, 1063 to 448\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1271 non-null   int64  \n",
      " 1   sex       1271 non-null   object \n",
      " 2   bmi       1271 non-null   float64\n",
      " 3   children  1271 non-null   int64  \n",
      " 4   smoker    1271 non-null   object \n",
      " 5   charges   1271 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 69.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(dataframe)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How many columns doe the dataset have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_cols = 6\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of the input variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'charges'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['age', 'sex', 'bmi', 'children', 'smoker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Which of the input columns are non-numeric or categorial variables ?**\n",
    "\n",
    "Hint: `sex` is one of them. List the columns that are not numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['sex','smoker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of output/target variable(s)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: (Optional) What is the minimum, maximum and average value of the `charges` column? Can you show the distribution of values in a graph?**\n",
    "Use this data visualization cheatsheet for referece: https://jovian.ml/aakashns/dataviz-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Charges:  1244.65726 \n",
      " Maximum Charges:  70147.470811 \n",
      " Average Charges:  14511.222486007871\n"
     ]
    }
   ],
   "source": [
    "#What is the minimum, maximum and average value of the charges column?\n",
    "min_Charges = dataframe['charges'].min()\n",
    "max_Charges = dataframe['charges'].max()\n",
    "Avg_Charges = dataframe['charges'].mean()\n",
    "print('Minimum Charges: ', min_Charges,'\\n Maximum Charges: ', max_Charges, '\\n Average Charges: ', Avg_Charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9cecd9b090>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can you show the distribution of values in a graph?\n",
    "import seaborn as sns\n",
    "sns.boxplot(y=dataframe.charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above boxplot for the column `charges` the lowest bar in graph represents `minimum` and the highest bar represents `maximum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to commit your notebook to Jovian after every step, so that you don't lose your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"manishshah120/02-insurance-linear\" on https://jovian.ml/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ml/manishshah120/02-insurance-linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/manishshah120/02-insurance-linear'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(message='Done with Step 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the dataset for training\n",
    "\n",
    "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    # Convert non-numeric categorical columns to numbers\n",
    "    for col in categorical_cols:\n",
    "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1[input_cols].to_numpy()\n",
    "    targets_array = dataframe1[output_cols].to_numpy()\n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) to understand how we're converting categorical variables into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[36.     ,  1.     , 27.73715,  3.     ,  0.     ],\n",
       "        [18.     ,  1.     , 51.5361 ,  0.     ,  0.     ],\n",
       "        [34.     ,  1.     , 21.7474 ,  2.     ,  0.     ],\n",
       "        ...,\n",
       "        [43.     ,  0.     , 34.6484 ,  2.     ,  0.     ],\n",
       "        [49.     ,  1.     , 34.7842 ,  0.     ,  0.     ],\n",
       "        [40.     ,  0.     , 28.712  ,  0.     ,  0.     ]]),\n",
       " array([[ 7203.014555],\n",
       "        [ 1279.80897 ],\n",
       "        [30113.495258],\n",
       "        ...,\n",
       "        [21059.034172],\n",
       "        [ 8936.84924 ],\n",
       "        [ 6502.0384  ]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
    "inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the array elemnets from float64 to float 32\n",
    "import numpy as np\n",
    "inputs = np.array(inputs_array,dtype='float32')\n",
    "targets = np.array(targets_array,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the inputs_array and targets_array into PyTorch tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype, targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[36.0000,  1.0000, 27.7372,  3.0000,  0.0000],\n",
       "         [18.0000,  1.0000, 51.5361,  0.0000,  0.0000],\n",
       "         [34.0000,  1.0000, 21.7474,  2.0000,  0.0000],\n",
       "         ...,\n",
       "         [43.0000,  0.0000, 34.6484,  2.0000,  0.0000],\n",
       "         [49.0000,  1.0000, 34.7842,  0.0000,  0.0000],\n",
       "         [40.0000,  0.0000, 28.7120,  0.0000,  0.0000]]),\n",
       " tensor([[ 7203.0146],\n",
       "         [ 1279.8090],\n",
       "         [30113.4961],\n",
       "         ...,\n",
       "         [21059.0332],\n",
       "         [ 8936.8496],\n",
       "         [ 6502.0386]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Pick a number between `0.1` and `0.2` to determine the fraction of data that will be used for creating the validation set. Then use `random_split` to create training & validation datasets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.15 # between 0.1 and 0.2\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create data loaders for training & validation.\n",
    "\n",
    "**Q: Pick a batch size for the data loader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a batch of data to verify everything is working fine so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[46.0000,  1.0000, 32.3447,  1.0000,  0.0000],\n",
      "        [55.0000,  1.0000, 20.8550,  1.0000,  0.0000],\n",
      "        [21.0000,  0.0000, 33.5620,  0.0000,  0.0000],\n",
      "        [44.0000,  1.0000, 33.2904,  1.0000,  0.0000],\n",
      "        [33.0000,  0.0000, 34.4641,  0.0000,  1.0000],\n",
      "        [35.0000,  1.0000, 37.4420,  1.0000,  0.0000],\n",
      "        [18.0000,  0.0000, 35.7445,  0.0000,  1.0000],\n",
      "        [51.0000,  0.0000, 17.5085,  0.0000,  0.0000],\n",
      "        [30.0000,  1.0000, 42.8934,  2.0000,  0.0000],\n",
      "        [38.0000,  0.0000, 38.9455,  0.0000,  0.0000],\n",
      "        [28.0000,  1.0000, 32.8054,  0.0000,  0.0000],\n",
      "        [56.0000,  1.0000, 31.1467,  1.0000,  0.0000],\n",
      "        [50.0000,  0.0000, 27.3152,  3.0000,  0.0000],\n",
      "        [64.0000,  0.0000, 30.3610,  2.0000,  1.0000],\n",
      "        [42.0000,  1.0000, 25.2879,  1.0000,  1.0000],\n",
      "        [43.0000,  1.0000, 19.5261,  2.0000,  1.0000],\n",
      "        [59.0000,  1.0000, 30.8363,  2.0000,  0.0000],\n",
      "        [36.0000,  1.0000, 28.0136,  3.0000,  0.0000],\n",
      "        [53.0000,  1.0000, 20.2730,  0.0000,  1.0000],\n",
      "        [49.0000,  1.0000, 30.4095,  1.0000,  0.0000],\n",
      "        [23.0000,  0.0000, 27.1600,  0.0000,  0.0000],\n",
      "        [54.0000,  0.0000, 28.0136,  2.0000,  0.0000],\n",
      "        [23.0000,  0.0000, 35.5699,  2.0000,  1.0000],\n",
      "        [60.0000,  1.0000, 38.7030,  0.0000,  1.0000],\n",
      "        [50.0000,  1.0000, 35.1140,  0.0000,  0.0000],\n",
      "        [52.0000,  1.0000, 31.7917,  3.0000,  0.0000],\n",
      "        [40.0000,  0.0000, 28.7120,  0.0000,  0.0000],\n",
      "        [35.0000,  0.0000, 36.9521,  2.0000,  0.0000],\n",
      "        [28.0000,  0.0000, 26.6750,  2.0000,  0.0000],\n",
      "        [34.0000,  0.0000, 26.6750,  1.0000,  0.0000],\n",
      "        [61.0000,  1.0000, 32.5289,  0.0000,  0.0000],\n",
      "        [20.0000,  0.0000, 21.1460,  0.0000,  1.0000]])\n",
      "targets: tensor([[ 9167.9033],\n",
      "        [11871.1562],\n",
      "        [ 2222.1946],\n",
      "        [ 7862.2202],\n",
      "        [60648.9414],\n",
      "        [ 5238.5620],\n",
      "        [39764.4336],\n",
      "        [10608.6777],\n",
      "        [ 4692.7822],\n",
      "        [ 5941.0786],\n",
      "        [21640.6699],\n",
      "        [12939.3008],\n",
      "        [11772.9062],\n",
      "        [52020.1602],\n",
      "        [42070.1523],\n",
      "        [20644.5117],\n",
      "        [14221.6699],\n",
      "        [ 7423.4502],\n",
      "        [23315.4004],\n",
      "        [10219.1533],\n",
      "        [14439.3447],\n",
      "        [13306.3164],\n",
      "        [42362.7930],\n",
      "        [52990.6953],\n",
      "        [ 9303.5996],\n",
      "        [12418.0205],\n",
      "        [ 6502.0386],\n",
      "        [27406.5508],\n",
      "        [22195.4375],\n",
      "        [ 5504.2383],\n",
      "        [14457.6699],\n",
      "        [22184.0703]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(\"inputs:\", xb)\n",
    "    print(\"targets:\", yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work by committing to Jovian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"manishshah120/02-insurance-linear\" on https://jovian.ml/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ml/manishshah120/02-insurance-linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/manishshah120/02-insurance-linear'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(message='Done with step 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Linear Regression Model\n",
    "\n",
    "Our model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(input_cols)\n",
    "output_size = len(output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**\n",
    "\n",
    "Hint: Think carefully about picking a good loss fuction (it's not cross entropy). Maybe try 2-3 of them and see which one works best. See https://pytorch.org/docs/stable/nn.functional.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)    # fill this (hint: use input_size & output_size defined above)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.linear(xb)    # fill this\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = F.l1_loss(out, targets)    # fill this\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(out, targets)    # fill this    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the weights and biases of the model using `model.parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0686,  0.2590, -0.0819,  0.3273, -0.0552]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2847], requires_grad=True)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final commit before we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jovian.commit(message='Done with Step 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model to fit the data\n",
    "\n",
    "To train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Use the `evaluate` function to calculate the loss on the validation set before training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 14531.1845703125}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader)    # Use the the evaluate function\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Train the model 4-5 times with different learning rates & for different number of epochs.**\n",
    "\n",
    "Hint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 7280.2700\n",
      "Epoch [40], val_loss: 7251.1460\n",
      "Epoch [60], val_loss: 7244.0552\n",
      "Epoch [80], val_loss: 7235.3774\n",
      "Epoch [100], val_loss: 7229.0308\n",
      "Epoch [120], val_loss: 7217.5117\n",
      "Epoch [140], val_loss: 7213.2954\n",
      "Epoch [160], val_loss: 7207.4751\n",
      "Epoch [180], val_loss: 7202.4136\n",
      "Epoch [200], val_loss: 7194.4961\n",
      "Epoch [220], val_loss: 7190.6147\n",
      "Epoch [240], val_loss: 7185.1313\n",
      "Epoch [260], val_loss: 7191.4946\n",
      "Epoch [280], val_loss: 7180.6562\n",
      "Epoch [300], val_loss: 7170.2466\n",
      "Epoch [320], val_loss: 7167.8472\n",
      "Epoch [340], val_loss: 7162.8501\n",
      "Epoch [360], val_loss: 7158.3301\n",
      "Epoch [380], val_loss: 7153.0703\n",
      "Epoch [400], val_loss: 7153.0024\n",
      "Epoch [420], val_loss: 7143.8984\n",
      "Epoch [440], val_loss: 7148.7539\n",
      "Epoch [460], val_loss: 7140.7817\n",
      "Epoch [480], val_loss: 7133.1196\n",
      "Epoch [500], val_loss: 7129.8022\n",
      "Epoch [520], val_loss: 7126.6406\n",
      "Epoch [540], val_loss: 7125.2817\n",
      "Epoch [560], val_loss: 7123.8086\n",
      "Epoch [580], val_loss: 7125.3242\n",
      "Epoch [600], val_loss: 7118.4497\n",
      "Epoch [620], val_loss: 7128.1343\n",
      "Epoch [640], val_loss: 7109.6108\n",
      "Epoch [660], val_loss: 7105.3013\n",
      "Epoch [680], val_loss: 7102.2378\n",
      "Epoch [700], val_loss: 7107.5679\n",
      "Epoch [720], val_loss: 7095.6812\n",
      "Epoch [740], val_loss: 7097.6694\n",
      "Epoch [760], val_loss: 7090.2349\n",
      "Epoch [780], val_loss: 7086.2407\n",
      "Epoch [800], val_loss: 7085.3823\n",
      "Epoch [820], val_loss: 7081.1099\n",
      "Epoch [840], val_loss: 7077.2251\n",
      "Epoch [860], val_loss: 7073.8984\n",
      "Epoch [880], val_loss: 7077.6392\n",
      "Epoch [900], val_loss: 7086.1870\n",
      "Epoch [920], val_loss: 7064.4531\n",
      "Epoch [940], val_loss: 7061.8691\n",
      "Epoch [960], val_loss: 7058.2407\n",
      "Epoch [980], val_loss: 7058.2266\n",
      "Epoch [1000], val_loss: 7067.3579\n",
      "Epoch [1020], val_loss: 7070.2954\n",
      "Epoch [1040], val_loss: 7045.8228\n",
      "Epoch [1060], val_loss: 7049.4390\n",
      "Epoch [1080], val_loss: 7040.7036\n",
      "Epoch [1100], val_loss: 7036.5806\n",
      "Epoch [1120], val_loss: 7036.6118\n",
      "Epoch [1140], val_loss: 7031.9253\n",
      "Epoch [1160], val_loss: 7027.4458\n",
      "Epoch [1180], val_loss: 7028.8555\n",
      "Epoch [1200], val_loss: 7020.7656\n",
      "Epoch [1220], val_loss: 7017.8521\n",
      "Epoch [1240], val_loss: 7021.0718\n",
      "Epoch [1260], val_loss: 7020.7969\n",
      "Epoch [1280], val_loss: 7009.0410\n",
      "Epoch [1300], val_loss: 7006.0796\n",
      "Epoch [1320], val_loss: 7007.8853\n",
      "Epoch [1340], val_loss: 6999.3696\n",
      "Epoch [1360], val_loss: 6996.3999\n",
      "Epoch [1380], val_loss: 6992.9370\n",
      "Epoch [1400], val_loss: 6999.5762\n",
      "Epoch [1420], val_loss: 6989.9062\n",
      "Epoch [1440], val_loss: 6984.3125\n",
      "Epoch [1460], val_loss: 6980.8179\n",
      "Epoch [1480], val_loss: 6977.1694\n",
      "Epoch [1500], val_loss: 6974.6450\n",
      "Epoch [1520], val_loss: 6971.0684\n",
      "Epoch [1540], val_loss: 6968.0269\n",
      "Epoch [1560], val_loss: 6970.0474\n",
      "Epoch [1580], val_loss: 6964.0522\n",
      "Epoch [1600], val_loss: 6969.0825\n",
      "Epoch [1620], val_loss: 6954.9995\n",
      "Epoch [1640], val_loss: 6953.2441\n",
      "Epoch [1660], val_loss: 6954.4434\n",
      "Epoch [1680], val_loss: 6946.6758\n",
      "Epoch [1700], val_loss: 6944.9790\n",
      "Epoch [1720], val_loss: 6940.5020\n",
      "Epoch [1740], val_loss: 6936.8354\n",
      "Epoch [1760], val_loss: 6936.1206\n",
      "Epoch [1780], val_loss: 6932.5894\n",
      "Epoch [1800], val_loss: 6927.9565\n",
      "Epoch [1820], val_loss: 6924.0181\n",
      "Epoch [1840], val_loss: 6924.0806\n",
      "Epoch [1860], val_loss: 6926.1758\n",
      "Epoch [1880], val_loss: 6914.9805\n",
      "Epoch [1900], val_loss: 6915.5601\n",
      "Epoch [1920], val_loss: 6911.6987\n",
      "Epoch [1940], val_loss: 6908.4565\n",
      "Epoch [1960], val_loss: 6902.0542\n",
      "Epoch [1980], val_loss: 6901.8867\n",
      "Epoch [2000], val_loss: 6899.3652\n",
      "Epoch [2020], val_loss: 6895.1157\n",
      "Epoch [2040], val_loss: 6891.0806\n",
      "Epoch [2060], val_loss: 6888.3008\n",
      "Epoch [2080], val_loss: 6887.2153\n",
      "Epoch [2100], val_loss: 6891.7065\n",
      "Epoch [2120], val_loss: 6877.0918\n",
      "Epoch [2140], val_loss: 6880.9067\n",
      "Epoch [2160], val_loss: 6871.0210\n",
      "Epoch [2180], val_loss: 6867.9507\n",
      "Epoch [2200], val_loss: 6866.8755\n",
      "Epoch [2220], val_loss: 6861.6704\n",
      "Epoch [2240], val_loss: 6858.4751\n",
      "Epoch [2260], val_loss: 6858.6343\n",
      "Epoch [2280], val_loss: 6852.5044\n",
      "Epoch [2300], val_loss: 6852.0703\n",
      "Epoch [2320], val_loss: 6850.6753\n",
      "Epoch [2340], val_loss: 6848.5767\n",
      "Epoch [2360], val_loss: 6846.2969\n",
      "Epoch [2380], val_loss: 6837.5151\n",
      "Epoch [2400], val_loss: 6848.4126\n",
      "Epoch [2420], val_loss: 6834.8726\n",
      "Epoch [2440], val_loss: 6830.9336\n",
      "Epoch [2460], val_loss: 6834.4702\n",
      "Epoch [2480], val_loss: 6826.3867\n",
      "Epoch [2500], val_loss: 6825.2651\n",
      "Epoch [2520], val_loss: 6818.2437\n",
      "Epoch [2540], val_loss: 6812.9771\n",
      "Epoch [2560], val_loss: 6811.6440\n",
      "Epoch [2580], val_loss: 6808.8477\n",
      "Epoch [2600], val_loss: 6809.9907\n",
      "Epoch [2620], val_loss: 6807.1699\n",
      "Epoch [2640], val_loss: 6801.4380\n",
      "Epoch [2660], val_loss: 6811.4004\n",
      "Epoch [2680], val_loss: 6791.1426\n",
      "Epoch [2700], val_loss: 6791.0649\n",
      "Epoch [2720], val_loss: 6789.0352\n",
      "Epoch [2740], val_loss: 6781.8638\n",
      "Epoch [2760], val_loss: 6780.4272\n",
      "Epoch [2780], val_loss: 6775.8423\n",
      "Epoch [2800], val_loss: 6773.0312\n",
      "Epoch [2820], val_loss: 6778.0649\n",
      "Epoch [2840], val_loss: 6769.1792\n",
      "Epoch [2860], val_loss: 6772.7397\n",
      "Epoch [2880], val_loss: 6762.5874\n",
      "Epoch [2900], val_loss: 6759.4653\n",
      "Epoch [2920], val_loss: 6754.6289\n",
      "Epoch [2940], val_loss: 6752.7207\n",
      "Epoch [2960], val_loss: 6749.0327\n",
      "Epoch [2980], val_loss: 6749.5176\n",
      "Epoch [3000], val_loss: 6745.9746\n",
      "Epoch [3020], val_loss: 6745.6035\n",
      "Epoch [3040], val_loss: 6736.4023\n",
      "Epoch [3060], val_loss: 6733.3901\n",
      "Epoch [3080], val_loss: 6733.4487\n",
      "Epoch [3100], val_loss: 6733.8203\n",
      "Epoch [3120], val_loss: 6727.3550\n",
      "Epoch [3140], val_loss: 6723.2915\n",
      "Epoch [3160], val_loss: 6718.5664\n",
      "Epoch [3180], val_loss: 6715.3735\n",
      "Epoch [3200], val_loss: 6723.3003\n",
      "Epoch [3220], val_loss: 6718.4868\n",
      "Epoch [3240], val_loss: 6706.3687\n",
      "Epoch [3260], val_loss: 6704.2935\n",
      "Epoch [3280], val_loss: 6707.0864\n",
      "Epoch [3300], val_loss: 6699.0601\n",
      "Epoch [3320], val_loss: 6697.3120\n",
      "Epoch [3340], val_loss: 6693.4761\n",
      "Epoch [3360], val_loss: 6690.3179\n",
      "Epoch [3380], val_loss: 6688.6641\n",
      "Epoch [3400], val_loss: 6685.0625\n",
      "Epoch [3420], val_loss: 6679.2700\n",
      "Epoch [3440], val_loss: 6682.2578\n",
      "Epoch [3460], val_loss: 6684.2051\n",
      "Epoch [3480], val_loss: 6670.3188\n",
      "Epoch [3500], val_loss: 6669.0229\n",
      "Epoch [3520], val_loss: 6664.5269\n",
      "Epoch [3540], val_loss: 6665.4160\n",
      "Epoch [3560], val_loss: 6662.4883\n",
      "Epoch [3580], val_loss: 6655.6465\n",
      "Epoch [3600], val_loss: 6652.4688\n",
      "Epoch [3620], val_loss: 6652.9609\n",
      "Epoch [3640], val_loss: 6649.0630\n",
      "Epoch [3660], val_loss: 6650.1421\n",
      "Epoch [3680], val_loss: 6641.6440\n",
      "Epoch [3700], val_loss: 6641.0532\n",
      "Epoch [3720], val_loss: 6636.2153\n",
      "Epoch [3740], val_loss: 6638.7993\n",
      "Epoch [3760], val_loss: 6632.0659\n",
      "Epoch [3780], val_loss: 6635.6348\n",
      "Epoch [3800], val_loss: 6625.4312\n",
      "Epoch [3820], val_loss: 6624.3599\n",
      "Epoch [3840], val_loss: 6617.5044\n",
      "Epoch [3860], val_loss: 6614.4648\n",
      "Epoch [3880], val_loss: 6611.9536\n",
      "Epoch [3900], val_loss: 6614.2192\n",
      "Epoch [3920], val_loss: 6608.5708\n",
      "Epoch [3940], val_loss: 6608.5493\n",
      "Epoch [3960], val_loss: 6601.6646\n",
      "Epoch [3980], val_loss: 6599.5308\n",
      "Epoch [4000], val_loss: 6598.4414\n",
      "Epoch [4020], val_loss: 6605.5288\n",
      "Epoch [4040], val_loss: 6591.0410\n",
      "Epoch [4060], val_loss: 6592.4321\n",
      "Epoch [4080], val_loss: 6590.1128\n",
      "Epoch [4100], val_loss: 6586.2573\n",
      "Epoch [4120], val_loss: 6593.6016\n",
      "Epoch [4140], val_loss: 6576.3726\n",
      "Epoch [4160], val_loss: 6571.0024\n",
      "Epoch [4180], val_loss: 6573.0024\n",
      "Epoch [4200], val_loss: 6565.1626\n",
      "Epoch [4220], val_loss: 6580.7505\n",
      "Epoch [4240], val_loss: 6561.5566\n",
      "Epoch [4260], val_loss: 6563.0215\n",
      "Epoch [4280], val_loss: 6554.8120\n",
      "Epoch [4300], val_loss: 6551.2715\n",
      "Epoch [4320], val_loss: 6550.5483\n",
      "Epoch [4340], val_loss: 6546.6094\n",
      "Epoch [4360], val_loss: 6541.9878\n",
      "Epoch [4380], val_loss: 6558.7500\n",
      "Epoch [4400], val_loss: 6538.7671\n",
      "Epoch [4420], val_loss: 6544.2427\n",
      "Epoch [4440], val_loss: 6530.5737\n",
      "Epoch [4460], val_loss: 6528.9673\n",
      "Epoch [4480], val_loss: 6531.1450\n",
      "Epoch [4500], val_loss: 6521.6855\n",
      "Epoch [4520], val_loss: 6519.0405\n",
      "Epoch [4540], val_loss: 6517.2817\n",
      "Epoch [4560], val_loss: 6519.0840\n",
      "Epoch [4580], val_loss: 6509.4780\n",
      "Epoch [4600], val_loss: 6509.0229\n",
      "Epoch [4620], val_loss: 6504.2573\n",
      "Epoch [4640], val_loss: 6503.8823\n",
      "Epoch [4660], val_loss: 6499.6616\n",
      "Epoch [4680], val_loss: 6501.0723\n",
      "Epoch [4700], val_loss: 6493.4087\n",
      "Epoch [4720], val_loss: 6492.7485\n",
      "Epoch [4740], val_loss: 6491.7871\n",
      "Epoch [4760], val_loss: 6487.7915\n",
      "Epoch [4780], val_loss: 6481.6719\n",
      "Epoch [4800], val_loss: 6487.9644\n",
      "Epoch [4820], val_loss: 6475.7466\n",
      "Epoch [4840], val_loss: 6475.0728\n",
      "Epoch [4860], val_loss: 6470.2671\n",
      "Epoch [4880], val_loss: 6473.6133\n",
      "Epoch [4900], val_loss: 6464.1890\n",
      "Epoch [4920], val_loss: 6462.0981\n",
      "Epoch [4940], val_loss: 6458.8521\n",
      "Epoch [4960], val_loss: 6456.3560\n",
      "Epoch [4980], val_loss: 6452.6343\n",
      "Epoch [5000], val_loss: 6451.5972\n",
      "Epoch [5020], val_loss: 6448.6831\n",
      "Epoch [5040], val_loss: 6444.6226\n",
      "Epoch [5060], val_loss: 6441.0996\n",
      "Epoch [5080], val_loss: 6438.0210\n",
      "Epoch [5100], val_loss: 6443.6836\n",
      "Epoch [5120], val_loss: 6433.3901\n",
      "Epoch [5140], val_loss: 6429.5039\n",
      "Epoch [5160], val_loss: 6435.4321\n",
      "Epoch [5180], val_loss: 6424.6548\n",
      "Epoch [5200], val_loss: 6420.9141\n",
      "Epoch [5220], val_loss: 6426.5669\n",
      "Epoch [5240], val_loss: 6416.9058\n",
      "Epoch [5260], val_loss: 6412.6323\n",
      "Epoch [5280], val_loss: 6409.6470\n",
      "Epoch [5300], val_loss: 6411.0801\n",
      "Epoch [5320], val_loss: 6404.2954\n",
      "Epoch [5340], val_loss: 6404.9429\n",
      "Epoch [5360], val_loss: 6404.0996\n",
      "Epoch [5380], val_loss: 6396.5845\n",
      "Epoch [5400], val_loss: 6394.9565\n",
      "Epoch [5420], val_loss: 6390.6655\n",
      "Epoch [5440], val_loss: 6387.6777\n",
      "Epoch [5460], val_loss: 6391.4297\n",
      "Epoch [5480], val_loss: 6381.2207\n",
      "Epoch [5500], val_loss: 6378.6704\n",
      "Epoch [5520], val_loss: 6406.2319\n",
      "Epoch [5540], val_loss: 6378.1797\n",
      "Epoch [5560], val_loss: 6370.1230\n",
      "Epoch [5580], val_loss: 6369.5142\n",
      "Epoch [5600], val_loss: 6372.4146\n",
      "Epoch [5620], val_loss: 6362.4844\n",
      "Epoch [5640], val_loss: 6367.0649\n",
      "Epoch [5660], val_loss: 6355.3320\n",
      "Epoch [5680], val_loss: 6354.0728\n",
      "Epoch [5700], val_loss: 6350.9478\n",
      "Epoch [5720], val_loss: 6347.1836\n",
      "Epoch [5740], val_loss: 6347.6504\n",
      "Epoch [5760], val_loss: 6341.6519\n",
      "Epoch [5780], val_loss: 6348.7163\n",
      "Epoch [5800], val_loss: 6336.4375\n",
      "Epoch [5820], val_loss: 6332.6479\n",
      "Epoch [5840], val_loss: 6346.1641\n",
      "Epoch [5860], val_loss: 6327.5952\n",
      "Epoch [5880], val_loss: 6333.8223\n",
      "Epoch [5900], val_loss: 6321.6035\n",
      "Epoch [5920], val_loss: 6326.3340\n",
      "Epoch [5940], val_loss: 6321.5483\n",
      "Epoch [5960], val_loss: 6316.7192\n",
      "Epoch [5980], val_loss: 6310.4575\n",
      "Epoch [6000], val_loss: 6308.3452\n",
      "Epoch [6020], val_loss: 6313.9233\n",
      "Epoch [6040], val_loss: 6302.0605\n",
      "Epoch [6060], val_loss: 6298.6655\n",
      "Epoch [6080], val_loss: 6302.9590\n",
      "Epoch [6100], val_loss: 6295.4004\n",
      "Epoch [6120], val_loss: 6290.4946\n",
      "Epoch [6140], val_loss: 6288.3872\n",
      "Epoch [6160], val_loss: 6284.7505\n",
      "Epoch [6180], val_loss: 6282.8931\n",
      "Epoch [6200], val_loss: 6283.5269\n",
      "Epoch [6220], val_loss: 6276.7056\n",
      "Epoch [6240], val_loss: 6276.8203\n",
      "Epoch [6260], val_loss: 6270.2612\n",
      "Epoch [6280], val_loss: 6277.1460\n",
      "Epoch [6300], val_loss: 6266.2852\n",
      "Epoch [6320], val_loss: 6264.6860\n",
      "Epoch [6340], val_loss: 6267.6577\n",
      "Epoch [6360], val_loss: 6263.9907\n",
      "Epoch [6380], val_loss: 6253.4238\n",
      "Epoch [6400], val_loss: 6257.3145\n",
      "Epoch [6420], val_loss: 6249.6211\n",
      "Epoch [6440], val_loss: 6244.7476\n",
      "Epoch [6460], val_loss: 6243.0806\n",
      "Epoch [6480], val_loss: 6241.4805\n",
      "Epoch [6500], val_loss: 6236.5625\n",
      "Epoch [6520], val_loss: 6233.3730\n",
      "Epoch [6540], val_loss: 6232.6016\n",
      "Epoch [6560], val_loss: 6239.5078\n",
      "Epoch [6580], val_loss: 6225.9238\n",
      "Epoch [6600], val_loss: 6228.2598\n",
      "Epoch [6620], val_loss: 6219.2495\n",
      "Epoch [6640], val_loss: 6216.3335\n",
      "Epoch [6660], val_loss: 6213.7329\n",
      "Epoch [6680], val_loss: 6211.3999\n",
      "Epoch [6700], val_loss: 6210.9077\n",
      "Epoch [6720], val_loss: 6207.1504\n",
      "Epoch [6740], val_loss: 6202.9028\n",
      "Epoch [6760], val_loss: 6199.7632\n",
      "Epoch [6780], val_loss: 6198.6934\n",
      "Epoch [6800], val_loss: 6195.5093\n",
      "Epoch [6820], val_loss: 6191.2183\n",
      "Epoch [6840], val_loss: 6187.7056\n",
      "Epoch [6860], val_loss: 6188.2056\n",
      "Epoch [6880], val_loss: 6184.5098\n",
      "Epoch [6900], val_loss: 6180.0864\n",
      "Epoch [6920], val_loss: 6177.9341\n",
      "Epoch [6940], val_loss: 6180.4868\n",
      "Epoch [6960], val_loss: 6173.4028\n",
      "Epoch [6980], val_loss: 6182.4380\n",
      "Epoch [7000], val_loss: 6166.7227\n",
      "Epoch [7020], val_loss: 6181.3335\n",
      "Epoch [7040], val_loss: 6166.9341\n",
      "Epoch [7060], val_loss: 6159.7578\n",
      "Epoch [7080], val_loss: 6154.0610\n",
      "Epoch [7100], val_loss: 6154.6899\n",
      "Epoch [7120], val_loss: 6148.8560\n",
      "Epoch [7140], val_loss: 6151.3579\n",
      "Epoch [7160], val_loss: 6148.1831\n",
      "Epoch [7180], val_loss: 6140.6328\n",
      "Epoch [7200], val_loss: 6143.9995\n",
      "Epoch [7220], val_loss: 6137.4683\n",
      "Epoch [7240], val_loss: 6133.3457\n",
      "Epoch [7260], val_loss: 6138.4634\n",
      "Epoch [7280], val_loss: 6129.9390\n",
      "Epoch [7300], val_loss: 6123.2827\n",
      "Epoch [7320], val_loss: 6121.6421\n",
      "Epoch [7340], val_loss: 6119.4478\n",
      "Epoch [7360], val_loss: 6116.3359\n",
      "Epoch [7380], val_loss: 6112.0776\n",
      "Epoch [7400], val_loss: 6115.8418\n",
      "Epoch [7420], val_loss: 6106.1777\n",
      "Epoch [7440], val_loss: 6103.3618\n",
      "Epoch [7460], val_loss: 6101.1851\n",
      "Epoch [7480], val_loss: 6098.3169\n",
      "Epoch [7500], val_loss: 6100.1831\n",
      "Epoch [7520], val_loss: 6095.0488\n",
      "Epoch [7540], val_loss: 6093.0620\n",
      "Epoch [7560], val_loss: 6087.9570\n",
      "Epoch [7580], val_loss: 6083.6938\n",
      "Epoch [7600], val_loss: 6083.9272\n",
      "Epoch [7620], val_loss: 6081.4370\n",
      "Epoch [7640], val_loss: 6076.3906\n",
      "Epoch [7660], val_loss: 6073.0815\n",
      "Epoch [7680], val_loss: 6069.9868\n",
      "Epoch [7700], val_loss: 6068.3013\n",
      "Epoch [7720], val_loss: 6065.8418\n",
      "Epoch [7740], val_loss: 6062.2603\n",
      "Epoch [7760], val_loss: 6058.2026\n",
      "Epoch [7780], val_loss: 6056.0859\n",
      "Epoch [7800], val_loss: 6052.3550\n",
      "Epoch [7820], val_loss: 6054.4351\n",
      "Epoch [7840], val_loss: 6047.8203\n",
      "Epoch [7860], val_loss: 6044.0898\n",
      "Epoch [7880], val_loss: 6042.3281\n",
      "Epoch [7900], val_loss: 6051.7769\n",
      "Epoch [7920], val_loss: 6037.0806\n",
      "Epoch [7940], val_loss: 6039.8628\n",
      "Epoch [7960], val_loss: 6032.9087\n",
      "Epoch [7980], val_loss: 6035.2485\n",
      "Epoch [8000], val_loss: 6024.7856\n",
      "Epoch [8020], val_loss: 6022.5259\n",
      "Epoch [8040], val_loss: 6018.7329\n",
      "Epoch [8060], val_loss: 6016.1855\n",
      "Epoch [8080], val_loss: 6016.3413\n",
      "Epoch [8100], val_loss: 6010.2583\n",
      "Epoch [8120], val_loss: 6008.3501\n",
      "Epoch [8140], val_loss: 6004.5288\n",
      "Epoch [8160], val_loss: 6001.5391\n",
      "Epoch [8180], val_loss: 6003.5864\n",
      "Epoch [8200], val_loss: 6000.0181\n",
      "Epoch [8220], val_loss: 5993.5015\n",
      "Epoch [8240], val_loss: 5990.6636\n",
      "Epoch [8260], val_loss: 5988.3149\n",
      "Epoch [8280], val_loss: 5984.5918\n",
      "Epoch [8300], val_loss: 5987.1030\n",
      "Epoch [8320], val_loss: 5979.9292\n",
      "Epoch [8340], val_loss: 5981.5796\n",
      "Epoch [8360], val_loss: 5982.4829\n",
      "Epoch [8380], val_loss: 5979.7739\n",
      "Epoch [8400], val_loss: 5968.2515\n",
      "Epoch [8420], val_loss: 5965.0630\n",
      "Epoch [8440], val_loss: 5974.7339\n",
      "Epoch [8460], val_loss: 5959.1431\n",
      "Epoch [8480], val_loss: 5956.4702\n",
      "Epoch [8500], val_loss: 5953.8101\n",
      "Epoch [8520], val_loss: 5952.8706\n",
      "Epoch [8540], val_loss: 5949.2739\n",
      "Epoch [8560], val_loss: 5966.6372\n",
      "Epoch [8580], val_loss: 5944.4375\n",
      "Epoch [8600], val_loss: 5941.3296\n",
      "Epoch [8620], val_loss: 5940.0508\n",
      "Epoch [8640], val_loss: 5947.5527\n",
      "Epoch [8660], val_loss: 5933.5488\n",
      "Epoch [8680], val_loss: 5946.7466\n",
      "Epoch [8700], val_loss: 5946.4438\n",
      "Epoch [8720], val_loss: 5925.2212\n",
      "Epoch [8740], val_loss: 5920.8833\n",
      "Epoch [8760], val_loss: 5929.8809\n",
      "Epoch [8780], val_loss: 5916.4604\n",
      "Epoch [8800], val_loss: 5919.4321\n",
      "Epoch [8820], val_loss: 5912.2944\n",
      "Epoch [8840], val_loss: 5910.0942\n",
      "Epoch [8860], val_loss: 5904.7485\n",
      "Epoch [8880], val_loss: 5910.3413\n",
      "Epoch [8900], val_loss: 5899.2056\n",
      "Epoch [8920], val_loss: 5897.0801\n",
      "Epoch [8940], val_loss: 5893.8594\n",
      "Epoch [8960], val_loss: 5889.6011\n",
      "Epoch [8980], val_loss: 5888.1704\n",
      "Epoch [9000], val_loss: 5887.9946\n",
      "Epoch [9020], val_loss: 5880.7056\n",
      "Epoch [9040], val_loss: 5885.3765\n",
      "Epoch [9060], val_loss: 5875.5977\n",
      "Epoch [9080], val_loss: 5881.6226\n",
      "Epoch [9100], val_loss: 5869.6582\n",
      "Epoch [9120], val_loss: 5868.0767\n",
      "Epoch [9140], val_loss: 5866.2905\n",
      "Epoch [9160], val_loss: 5863.2402\n",
      "Epoch [9180], val_loss: 5858.5757\n",
      "Epoch [9200], val_loss: 5859.7949\n",
      "Epoch [9220], val_loss: 5853.0532\n",
      "Epoch [9240], val_loss: 5855.5137\n",
      "Epoch [9260], val_loss: 5848.8574\n",
      "Epoch [9280], val_loss: 5850.2812\n",
      "Epoch [9300], val_loss: 5842.6660\n",
      "Epoch [9320], val_loss: 5840.9263\n",
      "Epoch [9340], val_loss: 5837.3633\n",
      "Epoch [9360], val_loss: 5835.0229\n",
      "Epoch [9380], val_loss: 5835.4409\n",
      "Epoch [9400], val_loss: 5828.4819\n",
      "Epoch [9420], val_loss: 5835.1519\n",
      "Epoch [9440], val_loss: 5821.9507\n",
      "Epoch [9460], val_loss: 5820.5938\n",
      "Epoch [9480], val_loss: 5820.1484\n",
      "Epoch [9500], val_loss: 5816.8789\n",
      "Epoch [9520], val_loss: 5812.0410\n",
      "Epoch [9540], val_loss: 5808.0488\n",
      "Epoch [9560], val_loss: 5815.9077\n",
      "Epoch [9580], val_loss: 5803.7168\n",
      "Epoch [9600], val_loss: 5805.1021\n",
      "Epoch [9620], val_loss: 5797.5547\n",
      "Epoch [9640], val_loss: 5796.0293\n",
      "Epoch [9660], val_loss: 5792.7671\n",
      "Epoch [9680], val_loss: 5789.5259\n",
      "Epoch [9700], val_loss: 5786.1914\n",
      "Epoch [9720], val_loss: 5783.0601\n",
      "Epoch [9740], val_loss: 5782.6968\n",
      "Epoch [9760], val_loss: 5783.0781\n",
      "Epoch [9780], val_loss: 5774.4331\n",
      "Epoch [9800], val_loss: 5772.9419\n",
      "Epoch [9820], val_loss: 5770.9409\n",
      "Epoch [9840], val_loss: 5766.4780\n",
      "Epoch [9860], val_loss: 5764.1753\n",
      "Epoch [9880], val_loss: 5766.1499\n",
      "Epoch [9900], val_loss: 5769.1465\n",
      "Epoch [9920], val_loss: 5762.8618\n",
      "Epoch [9940], val_loss: 5755.4785\n",
      "Epoch [9960], val_loss: 5748.7324\n",
      "Epoch [9980], val_loss: 5746.4175\n",
      "Epoch [10000], val_loss: 5746.5562\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 1e-1\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 5743.4116\n",
      "Epoch [40], val_loss: 5742.9888\n",
      "Epoch [60], val_loss: 5742.7441\n",
      "Epoch [80], val_loss: 5742.6152\n",
      "Epoch [100], val_loss: 5741.9634\n",
      "Epoch [120], val_loss: 5741.6499\n",
      "Epoch [140], val_loss: 5741.3062\n",
      "Epoch [160], val_loss: 5741.0215\n",
      "Epoch [180], val_loss: 5740.5161\n",
      "Epoch [200], val_loss: 5740.6489\n",
      "Epoch [220], val_loss: 5740.0376\n",
      "Epoch [240], val_loss: 5739.7925\n",
      "Epoch [260], val_loss: 5739.4292\n",
      "Epoch [280], val_loss: 5739.0640\n",
      "Epoch [300], val_loss: 5738.8062\n",
      "Epoch [320], val_loss: 5738.5840\n",
      "Epoch [340], val_loss: 5738.3462\n",
      "Epoch [360], val_loss: 5738.2007\n",
      "Epoch [380], val_loss: 5737.6333\n",
      "Epoch [400], val_loss: 5737.6870\n",
      "Epoch [420], val_loss: 5737.2866\n",
      "Epoch [440], val_loss: 5737.2007\n",
      "Epoch [460], val_loss: 5736.6772\n",
      "Epoch [480], val_loss: 5736.2935\n",
      "Epoch [500], val_loss: 5735.9824\n",
      "Epoch [520], val_loss: 5736.0645\n",
      "Epoch [540], val_loss: 5735.4180\n",
      "Epoch [560], val_loss: 5735.3071\n",
      "Epoch [580], val_loss: 5735.0425\n",
      "Epoch [600], val_loss: 5734.5190\n",
      "Epoch [620], val_loss: 5734.4897\n",
      "Epoch [640], val_loss: 5734.1270\n",
      "Epoch [660], val_loss: 5733.7769\n",
      "Epoch [680], val_loss: 5733.4858\n",
      "Epoch [700], val_loss: 5733.1558\n",
      "Epoch [720], val_loss: 5732.8843\n",
      "Epoch [740], val_loss: 5732.5767\n",
      "Epoch [760], val_loss: 5732.2896\n",
      "Epoch [780], val_loss: 5732.1499\n",
      "Epoch [800], val_loss: 5731.8828\n",
      "Epoch [820], val_loss: 5731.8418\n",
      "Epoch [840], val_loss: 5731.3276\n",
      "Epoch [860], val_loss: 5731.0425\n",
      "Epoch [880], val_loss: 5730.7852\n",
      "Epoch [900], val_loss: 5730.4712\n",
      "Epoch [920], val_loss: 5731.1167\n",
      "Epoch [940], val_loss: 5729.9136\n",
      "Epoch [960], val_loss: 5729.5659\n",
      "Epoch [980], val_loss: 5729.4614\n",
      "Epoch [1000], val_loss: 5729.0996\n",
      "Epoch [1020], val_loss: 5728.7378\n",
      "Epoch [1040], val_loss: 5728.4536\n",
      "Epoch [1060], val_loss: 5728.1929\n",
      "Epoch [1080], val_loss: 5727.9844\n",
      "Epoch [1100], val_loss: 5727.7148\n",
      "Epoch [1120], val_loss: 5727.4888\n",
      "Epoch [1140], val_loss: 5727.2241\n",
      "Epoch [1160], val_loss: 5726.9604\n",
      "Epoch [1180], val_loss: 5726.8657\n",
      "Epoch [1200], val_loss: 5726.2134\n",
      "Epoch [1220], val_loss: 5725.9976\n",
      "Epoch [1240], val_loss: 5725.7046\n",
      "Epoch [1260], val_loss: 5725.4688\n",
      "Epoch [1280], val_loss: 5725.1714\n",
      "Epoch [1300], val_loss: 5724.8687\n",
      "Epoch [1320], val_loss: 5724.6431\n",
      "Epoch [1340], val_loss: 5724.3472\n",
      "Epoch [1360], val_loss: 5724.1108\n",
      "Epoch [1380], val_loss: 5723.7534\n",
      "Epoch [1400], val_loss: 5723.5054\n",
      "Epoch [1420], val_loss: 5723.2617\n",
      "Epoch [1440], val_loss: 5722.9429\n",
      "Epoch [1460], val_loss: 5722.9292\n",
      "Epoch [1480], val_loss: 5722.4321\n",
      "Epoch [1500], val_loss: 5722.1548\n",
      "Epoch [1520], val_loss: 5721.8828\n",
      "Epoch [1540], val_loss: 5721.5820\n",
      "Epoch [1560], val_loss: 5721.3804\n",
      "Epoch [1580], val_loss: 5721.0962\n",
      "Epoch [1600], val_loss: 5720.7632\n",
      "Epoch [1620], val_loss: 5720.4941\n",
      "Epoch [1640], val_loss: 5720.3960\n",
      "Epoch [1660], val_loss: 5720.0142\n",
      "Epoch [1680], val_loss: 5719.8296\n",
      "Epoch [1700], val_loss: 5719.4683\n",
      "Epoch [1720], val_loss: 5719.1538\n",
      "Epoch [1740], val_loss: 5718.8867\n",
      "Epoch [1760], val_loss: 5718.6479\n",
      "Epoch [1780], val_loss: 5718.3516\n",
      "Epoch [1800], val_loss: 5718.0547\n",
      "Epoch [1820], val_loss: 5717.7988\n",
      "Epoch [1840], val_loss: 5717.5024\n",
      "Epoch [1860], val_loss: 5717.2695\n",
      "Epoch [1880], val_loss: 5717.0703\n",
      "Epoch [1900], val_loss: 5716.6890\n",
      "Epoch [1920], val_loss: 5716.4517\n",
      "Epoch [1940], val_loss: 5716.1328\n",
      "Epoch [1960], val_loss: 5715.8516\n",
      "Epoch [1980], val_loss: 5715.6226\n",
      "Epoch [2000], val_loss: 5715.3442\n",
      "Epoch [2020], val_loss: 5715.0122\n",
      "Epoch [2040], val_loss: 5714.7515\n",
      "Epoch [2060], val_loss: 5714.6919\n",
      "Epoch [2080], val_loss: 5714.2349\n",
      "Epoch [2100], val_loss: 5713.9526\n",
      "Epoch [2120], val_loss: 5713.9604\n",
      "Epoch [2140], val_loss: 5713.3306\n",
      "Epoch [2160], val_loss: 5713.0796\n",
      "Epoch [2180], val_loss: 5712.8589\n",
      "Epoch [2200], val_loss: 5712.5229\n",
      "Epoch [2220], val_loss: 5712.2212\n",
      "Epoch [2240], val_loss: 5711.9492\n",
      "Epoch [2260], val_loss: 5711.7090\n",
      "Epoch [2280], val_loss: 5711.4180\n",
      "Epoch [2300], val_loss: 5711.1880\n",
      "Epoch [2320], val_loss: 5710.9321\n",
      "Epoch [2340], val_loss: 5710.6733\n",
      "Epoch [2360], val_loss: 5710.4106\n",
      "Epoch [2380], val_loss: 5710.1138\n",
      "Epoch [2400], val_loss: 5709.8125\n",
      "Epoch [2420], val_loss: 5709.5649\n",
      "Epoch [2440], val_loss: 5709.2563\n",
      "Epoch [2460], val_loss: 5709.0234\n",
      "Epoch [2480], val_loss: 5708.6812\n",
      "Epoch [2500], val_loss: 5708.4766\n",
      "Epoch [2520], val_loss: 5708.3335\n",
      "Epoch [2540], val_loss: 5707.8628\n",
      "Epoch [2560], val_loss: 5707.5552\n",
      "Epoch [2580], val_loss: 5707.4634\n",
      "Epoch [2600], val_loss: 5707.0093\n",
      "Epoch [2620], val_loss: 5706.6836\n",
      "Epoch [2640], val_loss: 5706.4458\n",
      "Epoch [2660], val_loss: 5706.3823\n",
      "Epoch [2680], val_loss: 5705.8457\n",
      "Epoch [2700], val_loss: 5705.8667\n",
      "Epoch [2720], val_loss: 5705.3984\n",
      "Epoch [2740], val_loss: 5705.2749\n",
      "Epoch [2760], val_loss: 5704.7852\n",
      "Epoch [2780], val_loss: 5704.5327\n",
      "Epoch [2800], val_loss: 5704.2495\n",
      "Epoch [2820], val_loss: 5703.9644\n",
      "Epoch [2840], val_loss: 5703.7324\n",
      "Epoch [2860], val_loss: 5703.5308\n",
      "Epoch [2880], val_loss: 5703.1001\n",
      "Epoch [2900], val_loss: 5702.8750\n",
      "Epoch [2920], val_loss: 5702.6167\n",
      "Epoch [2940], val_loss: 5702.3413\n",
      "Epoch [2960], val_loss: 5702.0962\n",
      "Epoch [2980], val_loss: 5701.8208\n",
      "Epoch [3000], val_loss: 5701.7251\n",
      "Epoch [3020], val_loss: 5701.2812\n",
      "Epoch [3040], val_loss: 5700.9277\n",
      "Epoch [3060], val_loss: 5700.6753\n",
      "Epoch [3080], val_loss: 5700.4243\n",
      "Epoch [3100], val_loss: 5700.1958\n",
      "Epoch [3120], val_loss: 5699.8569\n",
      "Epoch [3140], val_loss: 5699.6851\n",
      "Epoch [3160], val_loss: 5699.3306\n",
      "Epoch [3180], val_loss: 5699.1602\n",
      "Epoch [3200], val_loss: 5698.7759\n",
      "Epoch [3220], val_loss: 5698.5020\n",
      "Epoch [3240], val_loss: 5698.2461\n",
      "Epoch [3260], val_loss: 5697.9336\n",
      "Epoch [3280], val_loss: 5697.6938\n",
      "Epoch [3300], val_loss: 5697.3970\n",
      "Epoch [3320], val_loss: 5697.1113\n",
      "Epoch [3340], val_loss: 5696.8340\n",
      "Epoch [3360], val_loss: 5696.8281\n",
      "Epoch [3380], val_loss: 5696.3569\n",
      "Epoch [3400], val_loss: 5696.0190\n",
      "Epoch [3420], val_loss: 5695.8315\n",
      "Epoch [3440], val_loss: 5695.5527\n",
      "Epoch [3460], val_loss: 5695.1421\n",
      "Epoch [3480], val_loss: 5694.8438\n",
      "Epoch [3500], val_loss: 5694.6245\n",
      "Epoch [3520], val_loss: 5694.3101\n",
      "Epoch [3540], val_loss: 5694.0073\n",
      "Epoch [3560], val_loss: 5693.7856\n",
      "Epoch [3580], val_loss: 5693.4985\n",
      "Epoch [3600], val_loss: 5693.1899\n",
      "Epoch [3620], val_loss: 5692.9219\n",
      "Epoch [3640], val_loss: 5692.6265\n",
      "Epoch [3660], val_loss: 5692.3237\n",
      "Epoch [3680], val_loss: 5692.2515\n",
      "Epoch [3700], val_loss: 5691.8027\n",
      "Epoch [3720], val_loss: 5691.7905\n",
      "Epoch [3740], val_loss: 5691.1880\n",
      "Epoch [3760], val_loss: 5691.0063\n",
      "Epoch [3780], val_loss: 5690.8555\n",
      "Epoch [3800], val_loss: 5690.4517\n",
      "Epoch [3820], val_loss: 5690.1562\n",
      "Epoch [3840], val_loss: 5689.9263\n",
      "Epoch [3860], val_loss: 5689.8384\n",
      "Epoch [3880], val_loss: 5689.2671\n",
      "Epoch [3900], val_loss: 5689.0688\n",
      "Epoch [3920], val_loss: 5688.7505\n",
      "Epoch [3940], val_loss: 5688.4351\n",
      "Epoch [3960], val_loss: 5688.1997\n",
      "Epoch [3980], val_loss: 5687.9863\n",
      "Epoch [4000], val_loss: 5687.7676\n",
      "Epoch [4020], val_loss: 5687.3374\n",
      "Epoch [4040], val_loss: 5687.2617\n",
      "Epoch [4060], val_loss: 5686.8262\n",
      "Epoch [4080], val_loss: 5686.6011\n",
      "Epoch [4100], val_loss: 5686.3823\n",
      "Epoch [4120], val_loss: 5686.0229\n",
      "Epoch [4140], val_loss: 5685.7852\n",
      "Epoch [4160], val_loss: 5685.4731\n",
      "Epoch [4180], val_loss: 5685.2036\n",
      "Epoch [4200], val_loss: 5684.9194\n",
      "Epoch [4220], val_loss: 5684.6792\n",
      "Epoch [4240], val_loss: 5684.3359\n",
      "Epoch [4260], val_loss: 5684.1074\n",
      "Epoch [4280], val_loss: 5683.8823\n",
      "Epoch [4300], val_loss: 5683.6196\n",
      "Epoch [4320], val_loss: 5683.3345\n",
      "Epoch [4340], val_loss: 5683.0234\n",
      "Epoch [4360], val_loss: 5682.8223\n",
      "Epoch [4380], val_loss: 5682.4702\n",
      "Epoch [4400], val_loss: 5682.2280\n",
      "Epoch [4420], val_loss: 5681.9062\n",
      "Epoch [4440], val_loss: 5681.5601\n",
      "Epoch [4460], val_loss: 5681.3340\n",
      "Epoch [4480], val_loss: 5681.1040\n",
      "Epoch [4500], val_loss: 5680.8188\n",
      "Epoch [4520], val_loss: 5680.5352\n",
      "Epoch [4540], val_loss: 5680.2573\n",
      "Epoch [4560], val_loss: 5680.1704\n",
      "Epoch [4580], val_loss: 5679.8340\n",
      "Epoch [4600], val_loss: 5679.4780\n",
      "Epoch [4620], val_loss: 5679.1919\n",
      "Epoch [4640], val_loss: 5678.8979\n",
      "Epoch [4660], val_loss: 5678.6499\n",
      "Epoch [4680], val_loss: 5678.3188\n",
      "Epoch [4700], val_loss: 5678.3027\n",
      "Epoch [4720], val_loss: 5677.7485\n",
      "Epoch [4740], val_loss: 5677.4976\n",
      "Epoch [4760], val_loss: 5677.2368\n",
      "Epoch [4780], val_loss: 5676.9243\n",
      "Epoch [4800], val_loss: 5676.7539\n",
      "Epoch [4820], val_loss: 5676.4155\n",
      "Epoch [4840], val_loss: 5676.1216\n",
      "Epoch [4860], val_loss: 5675.8901\n",
      "Epoch [4880], val_loss: 5675.7676\n",
      "Epoch [4900], val_loss: 5675.2720\n",
      "Epoch [4920], val_loss: 5675.0356\n",
      "Epoch [4940], val_loss: 5674.7368\n",
      "Epoch [4960], val_loss: 5674.4507\n",
      "Epoch [4980], val_loss: 5674.1987\n",
      "Epoch [5000], val_loss: 5673.9312\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 1e-2\n",
    "history2 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 5673.8960\n",
      "Epoch [40], val_loss: 5673.8687\n",
      "Epoch [60], val_loss: 5673.8413\n",
      "Epoch [80], val_loss: 5673.8179\n",
      "Epoch [100], val_loss: 5673.7915\n",
      "Epoch [120], val_loss: 5673.7671\n",
      "Epoch [140], val_loss: 5673.7437\n",
      "Epoch [160], val_loss: 5673.7168\n",
      "Epoch [180], val_loss: 5673.6880\n",
      "Epoch [200], val_loss: 5673.6582\n",
      "Epoch [220], val_loss: 5673.6284\n",
      "Epoch [240], val_loss: 5673.6011\n",
      "Epoch [260], val_loss: 5673.5728\n",
      "Epoch [280], val_loss: 5673.5449\n",
      "Epoch [300], val_loss: 5673.5220\n",
      "Epoch [320], val_loss: 5673.4946\n",
      "Epoch [340], val_loss: 5673.4707\n",
      "Epoch [360], val_loss: 5673.4497\n",
      "Epoch [380], val_loss: 5673.4185\n",
      "Epoch [400], val_loss: 5673.3906\n",
      "Epoch [420], val_loss: 5673.3667\n",
      "Epoch [440], val_loss: 5673.3379\n",
      "Epoch [460], val_loss: 5673.3091\n",
      "Epoch [480], val_loss: 5673.2812\n",
      "Epoch [500], val_loss: 5673.2563\n",
      "Epoch [520], val_loss: 5673.2339\n",
      "Epoch [540], val_loss: 5673.2046\n",
      "Epoch [560], val_loss: 5673.1777\n",
      "Epoch [580], val_loss: 5673.1582\n",
      "Epoch [600], val_loss: 5673.1255\n",
      "Epoch [620], val_loss: 5673.1021\n",
      "Epoch [640], val_loss: 5673.0762\n",
      "Epoch [660], val_loss: 5673.0488\n",
      "Epoch [680], val_loss: 5673.0220\n",
      "Epoch [700], val_loss: 5672.9985\n",
      "Epoch [720], val_loss: 5672.9746\n",
      "Epoch [740], val_loss: 5672.9492\n",
      "Epoch [760], val_loss: 5672.9224\n",
      "Epoch [780], val_loss: 5672.8945\n",
      "Epoch [800], val_loss: 5672.8711\n",
      "Epoch [820], val_loss: 5672.8462\n",
      "Epoch [840], val_loss: 5672.8237\n",
      "Epoch [860], val_loss: 5672.7949\n",
      "Epoch [880], val_loss: 5672.7700\n",
      "Epoch [900], val_loss: 5672.7383\n",
      "Epoch [920], val_loss: 5672.7163\n",
      "Epoch [940], val_loss: 5672.6875\n",
      "Epoch [960], val_loss: 5672.6680\n",
      "Epoch [980], val_loss: 5672.6367\n",
      "Epoch [1000], val_loss: 5672.6147\n",
      "Epoch [1020], val_loss: 5672.5884\n",
      "Epoch [1040], val_loss: 5672.5630\n",
      "Epoch [1060], val_loss: 5672.5415\n",
      "Epoch [1080], val_loss: 5672.5181\n",
      "Epoch [1100], val_loss: 5672.4897\n",
      "Epoch [1120], val_loss: 5672.4668\n",
      "Epoch [1140], val_loss: 5672.4395\n",
      "Epoch [1160], val_loss: 5672.4106\n",
      "Epoch [1180], val_loss: 5672.3872\n",
      "Epoch [1200], val_loss: 5672.3667\n",
      "Epoch [1220], val_loss: 5672.3379\n",
      "Epoch [1240], val_loss: 5672.3120\n",
      "Epoch [1260], val_loss: 5672.2871\n",
      "Epoch [1280], val_loss: 5672.2632\n",
      "Epoch [1300], val_loss: 5672.2378\n",
      "Epoch [1320], val_loss: 5672.2153\n",
      "Epoch [1340], val_loss: 5672.1895\n",
      "Epoch [1360], val_loss: 5672.1699\n",
      "Epoch [1380], val_loss: 5672.1499\n",
      "Epoch [1400], val_loss: 5672.1226\n",
      "Epoch [1420], val_loss: 5672.0981\n",
      "Epoch [1440], val_loss: 5672.0737\n",
      "Epoch [1460], val_loss: 5672.0522\n",
      "Epoch [1480], val_loss: 5672.0254\n",
      "Epoch [1500], val_loss: 5672.0015\n",
      "Epoch [1520], val_loss: 5671.9780\n",
      "Epoch [1540], val_loss: 5671.9624\n",
      "Epoch [1560], val_loss: 5671.9360\n",
      "Epoch [1580], val_loss: 5671.9155\n",
      "Epoch [1600], val_loss: 5671.8892\n",
      "Epoch [1620], val_loss: 5671.8647\n",
      "Epoch [1640], val_loss: 5671.8413\n",
      "Epoch [1660], val_loss: 5671.8218\n",
      "Epoch [1680], val_loss: 5671.8022\n",
      "Epoch [1700], val_loss: 5671.7793\n",
      "Epoch [1720], val_loss: 5671.7563\n",
      "Epoch [1740], val_loss: 5671.7402\n",
      "Epoch [1760], val_loss: 5671.7148\n",
      "Epoch [1780], val_loss: 5671.6934\n",
      "Epoch [1800], val_loss: 5671.6758\n",
      "Epoch [1820], val_loss: 5671.6484\n",
      "Epoch [1840], val_loss: 5671.6274\n",
      "Epoch [1860], val_loss: 5671.6050\n",
      "Epoch [1880], val_loss: 5671.5815\n",
      "Epoch [1900], val_loss: 5671.5649\n",
      "Epoch [1920], val_loss: 5671.5356\n",
      "Epoch [1940], val_loss: 5671.5156\n",
      "Epoch [1960], val_loss: 5671.5000\n",
      "Epoch [1980], val_loss: 5671.4805\n",
      "Epoch [2000], val_loss: 5671.4546\n",
      "Epoch [2020], val_loss: 5671.4331\n",
      "Epoch [2040], val_loss: 5671.4102\n",
      "Epoch [2060], val_loss: 5671.3965\n",
      "Epoch [2080], val_loss: 5671.3711\n",
      "Epoch [2100], val_loss: 5671.3516\n",
      "Epoch [2120], val_loss: 5671.3262\n",
      "Epoch [2140], val_loss: 5671.3071\n",
      "Epoch [2160], val_loss: 5671.2866\n",
      "Epoch [2180], val_loss: 5671.2642\n",
      "Epoch [2200], val_loss: 5671.2466\n",
      "Epoch [2220], val_loss: 5671.2266\n",
      "Epoch [2240], val_loss: 5671.2051\n",
      "Epoch [2260], val_loss: 5671.1875\n",
      "Epoch [2280], val_loss: 5671.1646\n",
      "Epoch [2300], val_loss: 5671.1401\n",
      "Epoch [2320], val_loss: 5671.1172\n",
      "Epoch [2340], val_loss: 5671.0972\n",
      "Epoch [2360], val_loss: 5671.0737\n",
      "Epoch [2380], val_loss: 5671.0547\n",
      "Epoch [2400], val_loss: 5671.0308\n",
      "Epoch [2420], val_loss: 5671.0117\n",
      "Epoch [2440], val_loss: 5670.9917\n",
      "Epoch [2460], val_loss: 5670.9668\n",
      "Epoch [2480], val_loss: 5670.9478\n",
      "Epoch [2500], val_loss: 5670.9282\n",
      "Epoch [2520], val_loss: 5670.8999\n",
      "Epoch [2540], val_loss: 5670.8794\n",
      "Epoch [2560], val_loss: 5670.8599\n",
      "Epoch [2580], val_loss: 5670.8379\n",
      "Epoch [2600], val_loss: 5670.8169\n",
      "Epoch [2620], val_loss: 5670.8008\n",
      "Epoch [2640], val_loss: 5670.7793\n",
      "Epoch [2660], val_loss: 5670.7539\n",
      "Epoch [2680], val_loss: 5670.7290\n",
      "Epoch [2700], val_loss: 5670.7144\n",
      "Epoch [2720], val_loss: 5670.6919\n",
      "Epoch [2740], val_loss: 5670.6680\n",
      "Epoch [2760], val_loss: 5670.6489\n",
      "Epoch [2780], val_loss: 5670.6304\n",
      "Epoch [2800], val_loss: 5670.6113\n",
      "Epoch [2820], val_loss: 5670.5864\n",
      "Epoch [2840], val_loss: 5670.5610\n",
      "Epoch [2860], val_loss: 5670.5410\n",
      "Epoch [2880], val_loss: 5670.5190\n",
      "Epoch [2900], val_loss: 5670.5005\n",
      "Epoch [2920], val_loss: 5670.4819\n",
      "Epoch [2940], val_loss: 5670.4609\n",
      "Epoch [2960], val_loss: 5670.4419\n",
      "Epoch [2980], val_loss: 5670.4243\n",
      "Epoch [3000], val_loss: 5670.4019\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "lr = 1e-3\n",
    "history3 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 5670.4009\n",
      "Epoch [40], val_loss: 5670.4009\n",
      "Epoch [60], val_loss: 5670.4019\n",
      "Epoch [80], val_loss: 5670.4009\n",
      "Epoch [100], val_loss: 5670.4004\n",
      "Epoch [120], val_loss: 5670.3999\n",
      "Epoch [140], val_loss: 5670.3999\n",
      "Epoch [160], val_loss: 5670.3989\n",
      "Epoch [180], val_loss: 5670.3989\n",
      "Epoch [200], val_loss: 5670.3984\n",
      "Epoch [220], val_loss: 5670.3984\n",
      "Epoch [240], val_loss: 5670.3970\n",
      "Epoch [260], val_loss: 5670.3965\n",
      "Epoch [280], val_loss: 5670.3960\n",
      "Epoch [300], val_loss: 5670.3960\n",
      "Epoch [320], val_loss: 5670.3965\n",
      "Epoch [340], val_loss: 5670.3960\n",
      "Epoch [360], val_loss: 5670.3945\n",
      "Epoch [380], val_loss: 5670.3940\n",
      "Epoch [400], val_loss: 5670.3940\n",
      "Epoch [420], val_loss: 5670.3940\n",
      "Epoch [440], val_loss: 5670.3926\n",
      "Epoch [460], val_loss: 5670.3921\n",
      "Epoch [480], val_loss: 5670.3926\n",
      "Epoch [500], val_loss: 5670.3911\n",
      "Epoch [520], val_loss: 5670.3906\n",
      "Epoch [540], val_loss: 5670.3892\n",
      "Epoch [560], val_loss: 5670.3892\n",
      "Epoch [580], val_loss: 5670.3887\n",
      "Epoch [600], val_loss: 5670.3887\n",
      "Epoch [620], val_loss: 5670.3882\n",
      "Epoch [640], val_loss: 5670.3882\n",
      "Epoch [660], val_loss: 5670.3872\n",
      "Epoch [680], val_loss: 5670.3867\n",
      "Epoch [700], val_loss: 5670.3872\n",
      "Epoch [720], val_loss: 5670.3867\n",
      "Epoch [740], val_loss: 5670.3867\n",
      "Epoch [760], val_loss: 5670.3862\n",
      "Epoch [780], val_loss: 5670.3862\n",
      "Epoch [800], val_loss: 5670.3862\n",
      "Epoch [820], val_loss: 5670.3853\n",
      "Epoch [840], val_loss: 5670.3848\n",
      "Epoch [860], val_loss: 5670.3843\n",
      "Epoch [880], val_loss: 5670.3843\n",
      "Epoch [900], val_loss: 5670.3843\n",
      "Epoch [920], val_loss: 5670.3843\n",
      "Epoch [940], val_loss: 5670.3833\n",
      "Epoch [960], val_loss: 5670.3828\n",
      "Epoch [980], val_loss: 5670.3828\n",
      "Epoch [1000], val_loss: 5670.3813\n",
      "Epoch [1020], val_loss: 5670.3823\n",
      "Epoch [1040], val_loss: 5670.3813\n",
      "Epoch [1060], val_loss: 5670.3813\n",
      "Epoch [1080], val_loss: 5670.3809\n",
      "Epoch [1100], val_loss: 5670.3804\n",
      "Epoch [1120], val_loss: 5670.3804\n",
      "Epoch [1140], val_loss: 5670.3794\n",
      "Epoch [1160], val_loss: 5670.3794\n",
      "Epoch [1180], val_loss: 5670.3789\n",
      "Epoch [1200], val_loss: 5670.3784\n",
      "Epoch [1220], val_loss: 5670.3784\n",
      "Epoch [1240], val_loss: 5670.3774\n",
      "Epoch [1260], val_loss: 5670.3774\n",
      "Epoch [1280], val_loss: 5670.3770\n",
      "Epoch [1300], val_loss: 5670.3774\n",
      "Epoch [1320], val_loss: 5670.3770\n",
      "Epoch [1340], val_loss: 5670.3770\n",
      "Epoch [1360], val_loss: 5670.3770\n",
      "Epoch [1380], val_loss: 5670.3770\n",
      "Epoch [1400], val_loss: 5670.3765\n",
      "Epoch [1420], val_loss: 5670.3755\n",
      "Epoch [1440], val_loss: 5670.3765\n",
      "Epoch [1460], val_loss: 5670.3750\n",
      "Epoch [1480], val_loss: 5670.3750\n",
      "Epoch [1500], val_loss: 5670.3745\n",
      "Epoch [1520], val_loss: 5670.3750\n",
      "Epoch [1540], val_loss: 5670.3750\n",
      "Epoch [1560], val_loss: 5670.3745\n",
      "Epoch [1580], val_loss: 5670.3730\n",
      "Epoch [1600], val_loss: 5670.3730\n",
      "Epoch [1620], val_loss: 5670.3730\n",
      "Epoch [1640], val_loss: 5670.3726\n",
      "Epoch [1660], val_loss: 5670.3735\n",
      "Epoch [1680], val_loss: 5670.3735\n",
      "Epoch [1700], val_loss: 5670.3730\n",
      "Epoch [1720], val_loss: 5670.3730\n",
      "Epoch [1740], val_loss: 5670.3730\n",
      "Epoch [1760], val_loss: 5670.3735\n",
      "Epoch [1780], val_loss: 5670.3726\n",
      "Epoch [1800], val_loss: 5670.3730\n",
      "Epoch [1820], val_loss: 5670.3730\n",
      "Epoch [1840], val_loss: 5670.3730\n",
      "Epoch [1860], val_loss: 5670.3735\n",
      "Epoch [1880], val_loss: 5670.3730\n",
      "Epoch [1900], val_loss: 5670.3730\n",
      "Epoch [1920], val_loss: 5670.3726\n",
      "Epoch [1940], val_loss: 5670.3716\n",
      "Epoch [1960], val_loss: 5670.3726\n",
      "Epoch [1980], val_loss: 5670.3726\n",
      "Epoch [2000], val_loss: 5670.3726\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "lr = 1e-4\n",
    "history4 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 5670.3716\n",
      "Epoch [40], val_loss: 5670.3726\n",
      "Epoch [60], val_loss: 5670.3726\n",
      "Epoch [80], val_loss: 5670.3716\n",
      "Epoch [100], val_loss: 5670.3726\n",
      "Epoch [120], val_loss: 5670.3716\n",
      "Epoch [140], val_loss: 5670.3726\n",
      "Epoch [160], val_loss: 5670.3716\n",
      "Epoch [180], val_loss: 5670.3726\n",
      "Epoch [200], val_loss: 5670.3726\n",
      "Epoch [220], val_loss: 5670.3716\n",
      "Epoch [240], val_loss: 5670.3716\n",
      "Epoch [260], val_loss: 5670.3711\n",
      "Epoch [280], val_loss: 5670.3716\n",
      "Epoch [300], val_loss: 5670.3716\n",
      "Epoch [320], val_loss: 5670.3711\n",
      "Epoch [340], val_loss: 5670.3711\n",
      "Epoch [360], val_loss: 5670.3711\n",
      "Epoch [380], val_loss: 5670.3716\n",
      "Epoch [400], val_loss: 5670.3716\n",
      "Epoch [420], val_loss: 5670.3716\n",
      "Epoch [440], val_loss: 5670.3716\n",
      "Epoch [460], val_loss: 5670.3711\n",
      "Epoch [480], val_loss: 5670.3711\n",
      "Epoch [500], val_loss: 5670.3706\n",
      "Epoch [520], val_loss: 5670.3711\n",
      "Epoch [540], val_loss: 5670.3711\n",
      "Epoch [560], val_loss: 5670.3711\n",
      "Epoch [580], val_loss: 5670.3711\n",
      "Epoch [600], val_loss: 5670.3711\n",
      "Epoch [620], val_loss: 5670.3706\n",
      "Epoch [640], val_loss: 5670.3711\n",
      "Epoch [660], val_loss: 5670.3711\n",
      "Epoch [680], val_loss: 5670.3711\n",
      "Epoch [700], val_loss: 5670.3711\n",
      "Epoch [720], val_loss: 5670.3711\n",
      "Epoch [740], val_loss: 5670.3706\n",
      "Epoch [760], val_loss: 5670.3706\n",
      "Epoch [780], val_loss: 5670.3711\n",
      "Epoch [800], val_loss: 5670.3711\n",
      "Epoch [820], val_loss: 5670.3711\n",
      "Epoch [840], val_loss: 5670.3711\n",
      "Epoch [860], val_loss: 5670.3706\n",
      "Epoch [880], val_loss: 5670.3711\n",
      "Epoch [900], val_loss: 5670.3711\n",
      "Epoch [920], val_loss: 5670.3706\n",
      "Epoch [940], val_loss: 5670.3706\n",
      "Epoch [960], val_loss: 5670.3711\n",
      "Epoch [980], val_loss: 5670.3696\n",
      "Epoch [1000], val_loss: 5670.3706\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-5\n",
    "history5 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the final validation loss of your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 5670.3706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log the final validation loss to Jovian and commit the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Metrics logged.\n"
     ]
    }
   ],
   "source": [
    "jovian.log_metrics(val_loss=val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"manishshah120/02-insurance-linear\" on https://jovian.ml/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ml/manishshah120/02-insurance-linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/manishshah120/02-insurance-linear'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(message='Done with step 4 with val_loss4009')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scroll back up, re-initialize the model, and try different set of values for batch size, number of epochs, learning rate etc. Commit each experiment and use the \"Compare\" and \"View Diff\" options on Jovian to compare the different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions using the trained model\n",
    "\n",
    "**Q: Complete the following function definition to make predictions on a single input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)               # fill this\n",
    "    prediction = predictions[0].detach()\n",
    "    print(\"Input:\", input)\n",
    "    print(\"Target:\", target)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([53.0000,  1.0000, 25.6177,  2.0000,  0.0000])\n",
      "Target: tensor([12368.8145])\n",
      "Prediction: tensor([12307.2930])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[0]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([22.0000,  0.0000, 23.5710,  0.0000,  0.0000])\n",
      "Target: tensor([2365.5159])\n",
      "Prediction: tensor([3315.4238])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[10]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([57.0000,  0.0000, 30.8703,  0.0000,  0.0000])\n",
      "Target: tensor([13026.8857])\n",
      "Prediction: tensor([12937.1035])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[23]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you happy with your model's predictions? Try to improve them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Step 6: Try another dataset & blog about it\n",
    "\n",
    "While this last step is optional for the submission of your assignment, we highly recommend that you do it. Try to clean up & replicate this notebook (or [this one](https://jovian.ml/aakashns/housing-linear-minimal), or [this one](https://jovian.ml/aakashns/mnist-logistic-minimal) ) for a different linear regression or logistic regression problem. This will help solidify your understanding, and give you a chance to differentiate the generic patters in machine learning from problem-specific details.\n",
    "\n",
    "Here are some sources to find good datasets:\n",
    "\n",
    "- https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/\n",
    "- https://www.kaggle.com/rtatman/datasets-for-regression-analysis\n",
    "- https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table\n",
    "- https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\n",
    "- https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "- https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "We also recommend that you write a blog about your approach to the problem. Here is a suggested structure for your post (feel free to experiment with it):\n",
    "\n",
    "- Interesting title & subtitle\n",
    "- Overview of what the blog covers (which dataset, linear regression or logistic regression, intro to PyTorch)\n",
    "- Downloading & exploring the data\n",
    "- Preparing the data for training\n",
    "- Creating a model using PyTorch\n",
    "- Training the model to fit the data\n",
    "- Your thoughts on how to experiment with different hyperparmeters to reduce loss\n",
    "- Making predictions using the model\n",
    "\n",
    "As with the previous assignment, you can [embed Juptyer notebook cells & outputs from Jovian](https://medium.com/jovianml/share-and-embed-jupyter-notebooks-online-with-jovian-ml-df709a03064e) into your blog. \n",
    "\n",
    "Don't forget to share your work on the forum: https://jovian.ml/forum/t/share-your-work-here-assignment-2/4931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"manishshah120/02-insurance-linear\" on https://jovian.ml/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ml/manishshah120/02-insurance-linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/manishshah120/02-insurance-linear'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(message='Done with Step 5 and Assignment Completed Finally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**----------------THE END------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
